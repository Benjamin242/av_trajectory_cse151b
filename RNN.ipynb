{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle5\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"C:/Users/Administrator/cse151b-spring2022/argo2/\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "    inputs = pickle5.load(open(f_in, \"rb\"))\n",
    "    inputs = np.asarray(inputs)\n",
    "    \n",
    "    resampled = []\n",
    "    for i in inputs:\n",
    "        resampled.append(signal.resample(i, 60))\n",
    "        \n",
    "\n",
    "    resampled = np.array(resampled)\n",
    "    resampled =resampled / 20000\n",
    "    #print(resampled.shape)\n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle5.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)\n",
    "        outputs = outputs / 20000\n",
    "\n",
    "        return torch.from_numpy(resampled).float(), torch.from_numpy(outputs).float()\n",
    "\n",
    "    if split==\"test\":\n",
    "    \n",
    "        return torch.from_numpy(resampled).float(), torch.from_numpy(np.array([]))\n",
    "\n",
    "def transform(tup):\n",
    "    transed = (signal.resample(tup[0], 60), tup[1])\n",
    "    return transed / 20000\n",
    "\n",
    "    \n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None, device='cpu'):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "        self.inputs = self.inputs.to(device)\n",
    "        self.outputs = self.outputs.to(device)\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.split == 'train':\n",
    "            data = (self.inputs[idx], self.outputs[idx])\n",
    "        if self.split == 'test':\n",
    "            data = self.inputs[idx]\n",
    "            \n",
    "        if self.transform:\n",
    "            #data[0] = self.transform(data[0])\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# intialize a dataset\n",
    "city = 'palo-alto' \n",
    "split = 'train'\n",
    "train_dataset  = ArgoverseDataset(city = city, split = split, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 256  # batch size \n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ArgoverseDataset(city=city, split='test', device=device)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "hidden_size = 16\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers = self.num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        #h0 = torch.zeros(1, input.size(0), self.hidden_size).float().to(device)\n",
    "        #c0 = torch.zeros(1, input.size(0), self.hidden_size).float().to(device)\n",
    "        input = self.dropout(input)\n",
    "        out, hid = self.lstm(input)\n",
    "        return out, hid\n",
    "\n",
    "    # def initHidden(self):\n",
    "    #     return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout_p=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=self.num_layers, batch_first=True)\n",
    "        # self.attn = nn.Linear(self.hidden_size*2, 60)\n",
    "        # self.attn_combine = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
    "        # self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        output, hidden = self.lstm(input, hidden)\n",
    "        output = self.out(output)\n",
    "        #output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    # def init_state(self, encoded):\n",
    "    #     #torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    #     return encoded[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = EncoderRNN(input_size=2, hidden_size=hidden_size, num_layers=2)\n",
    "# decoder = DecoderRNN(hidden_size=hidden_size, output_size=2, num_layers=2)\n",
    "# encoder.to(device)\n",
    "# decoder.to(device)\n",
    "# optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_set):\n",
    "    encoder = EncoderRNN(input_size=2, hidden_size=hidden_size, num_layers=2)\n",
    "    decoder = DecoderRNN(hidden_size=hidden_size, output_size=2, num_layers=2)\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in train_set:\n",
    "            #print(f'input x shape: {x.shape}')\n",
    "            #print(f'input y shape: {y.shape}')\n",
    "            encoder.train()\n",
    "            optimizer.zero_grad()\n",
    "            encoded, hid = encoder(x)\n",
    "            #print(f'encoded shape: {encoded.shape}')\n",
    "            outputs, hid = decoder(encoded, hid)\n",
    "            \n",
    "            #print(f'decoded shape: {outputs.shape}')\n",
    "            train_loss = criterion(outputs, y)\n",
    "            #print(outputs)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        losses.append(float(train_loss.item()))    \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'epoch {epoch}, loss: {train_loss.item()}')\n",
    "\n",
    "    return encoder, decoder, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(y = losses, x = range(epochs))\n",
    "#plt.savefig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predictions(test_set, enc, dec, city):\n",
    "    preds = torch.tensor([])\n",
    "    for x in test_set:\n",
    "        enc.eval()\n",
    "        dec.eval()\n",
    "\n",
    "        encoded, hid = enc(x)\n",
    "        outputs, hid = dec(encoded, hid)\n",
    "        outputs = outputs.to('cpu').detach()\n",
    "        outputs = outputs * 20000\n",
    "        preds = torch.cat((preds, outputs), dim=0)\n",
    "        #preds.append(outputs)    \n",
    "\n",
    "    preds_2 = []\n",
    "    for i in preds:\n",
    "        preds_2.append(i.flatten())\n",
    "\n",
    "    df = pd.DataFrame(preds_2)\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: x.item())\n",
    "    df.columns = ['v' + str(i) for i in (range(120))]\n",
    "    df['ID'] = [str(i) + '_' + city for i in (range(len(test_set.dataset)))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.005131986457854509\n",
      "epoch 10, loss: 0.0032755788415670395\n",
      "epoch 20, loss: 0.002467469545081258\n",
      "epoch 30, loss: 0.0023509596940129995\n",
      "epoch 40, loss: 0.0022356777917593718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\data_analysis\\lib\\site-packages\\ipykernel_launcher.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.savefig(*args, **kwargs)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_sz = 256  # batch size \n",
    "train_dataset = ArgoverseDataset(city = 'palo-alto', split = 'train', device=device)\n",
    "train_loader_palo = DataLoader(train_dataset,batch_size=batch_sz)\n",
    "\n",
    "test_dataset = ArgoverseDataset(city='palo-alto', split='test', device=device)\n",
    "test_loader_palo = DataLoader(test_dataset,batch_size=batch_sz)\n",
    "\n",
    "\n",
    "enc, dec, losses = train(train_loader_palo)\n",
    "out_palo = test_predictions(test_loader_palo, enc, dec, 'palo-alto')\n",
    "\n",
    "sns.lineplot(y = losses, x = range(epochs))\n",
    "plt.savefig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.009244410321116447\n",
      "epoch 10, loss: 0.008229849860072136\n",
      "epoch 20, loss: 0.008089055307209492\n",
      "epoch 30, loss: 0.00805894285440445\n",
      "epoch 40, loss: 0.00805406179279089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\data_analysis\\lib\\site-packages\\ipykernel_launcher.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ArgoverseDataset(city = 'austin', split = 'train', device=device)\n",
    "train_loader_austin = DataLoader(train_dataset,batch_size=batch_sz)\n",
    "\n",
    "test_dataset = ArgoverseDataset(city='austin', split='test', device=device)\n",
    "test_loader_austin = DataLoader(test_dataset,batch_size=batch_sz)\n",
    "\n",
    "\n",
    "enc, dec, losses = train(train_loader_austin)\n",
    "out_austin = test_predictions(test_loader_austin, enc, dec, 'austin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.025771314278244972\n",
      "epoch 10, loss: 0.02177749015390873\n",
      "epoch 20, loss: 0.021472187712788582\n",
      "epoch 30, loss: 0.02138374373316765\n",
      "epoch 40, loss: 0.021365245804190636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\data_analysis\\lib\\site-packages\\ipykernel_launcher.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ArgoverseDataset(city = 'miami', split = 'train', device=device)\n",
    "train_loader_miami = DataLoader(train_dataset,batch_size=batch_sz)\n",
    "\n",
    "test_dataset = ArgoverseDataset(city='miami', split='test', device=device)\n",
    "test_loader_miami = DataLoader(test_dataset,batch_size=batch_sz)\n",
    "\n",
    "\n",
    "enc, dec, losses = train(train_loader_miami)\n",
    "out_miami = test_predictions(test_loader_miami, enc, dec, 'miami')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.010783861391246319\n",
      "epoch 10, loss: 0.008646922186017036\n",
      "epoch 20, loss: 0.00860297679901123\n",
      "epoch 30, loss: 0.008568313904106617\n",
      "epoch 40, loss: 0.008547035977244377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\data_analysis\\lib\\site-packages\\ipykernel_launcher.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ArgoverseDataset(city = 'pittsburgh', split = 'train', device=device)\n",
    "train_loader_pittsburgh = DataLoader(train_dataset,batch_size=batch_sz)\n",
    "\n",
    "test_dataset = ArgoverseDataset(city='pittsburgh', split='test', device=device)\n",
    "test_loader_pittsburgh = DataLoader(test_dataset,batch_size=batch_sz)\n",
    "\n",
    "\n",
    "enc, dec, losses = train(train_loader_pittsburgh)\n",
    "out_pittsburgh = test_predictions(test_loader_pittsburgh, enc, dec, 'pittsburgh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.06876182556152344\n",
      "epoch 10, loss: 0.06412059813737869\n",
      "epoch 20, loss: 0.06400150060653687\n",
      "epoch 30, loss: 0.0638786181807518\n",
      "epoch 40, loss: 0.06377626210451126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\data_analysis\\lib\\site-packages\\ipykernel_launcher.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ArgoverseDataset(city = 'dearborn', split = 'train', device=device)\n",
    "train_loader_dearborn = DataLoader(train_dataset,batch_size=batch_sz)\n",
    "\n",
    "test_dataset = ArgoverseDataset(city='dearborn', split='test', device=device)\n",
    "test_loader_dearborn = DataLoader(test_dataset,batch_size=batch_sz)\n",
    "\n",
    "\n",
    "enc, dec, losses = train(train_loader_dearborn)\n",
    "out_dearborn = test_predictions(test_loader_dearborn, enc, dec, 'dearborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.016487132757902145\n",
      "epoch 10, loss: 0.015472347848117352\n",
      "epoch 20, loss: 0.014981640502810478\n",
      "epoch 30, loss: 0.014875959604978561\n",
      "epoch 40, loss: 0.014822814613580704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\data_analysis\\lib\\site-packages\\ipykernel_launcher.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ArgoverseDataset(city = 'washington-dc', split = 'train', device=device)\n",
    "train_loader_washington = DataLoader(train_dataset,batch_size=batch_sz)\n",
    "\n",
    "test_dataset = ArgoverseDataset(city='washington-dc', split='test', device=device)\n",
    "test_loader_washington = DataLoader(test_dataset,batch_size=batch_sz)\n",
    "\n",
    "\n",
    "enc, dec, losses = train(train_loader_washington)\n",
    "out_washington = test_predictions(test_loader_washington, enc, dec, 'washington-dc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329.7426551580429"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.016487132757902145 * 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_result = pd.concat([out_austin, out_miami, out_pittsburgh, out_dearborn, out_washington, out_palo], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v0</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v111</th>\n",
       "      <th>v112</th>\n",
       "      <th>v113</th>\n",
       "      <th>v114</th>\n",
       "      <th>v115</th>\n",
       "      <th>v116</th>\n",
       "      <th>v117</th>\n",
       "      <th>v118</th>\n",
       "      <th>v119</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4594.164551</td>\n",
       "      <td>593.487000</td>\n",
       "      <td>4679.035156</td>\n",
       "      <td>1782.869751</td>\n",
       "      <td>4877.182129</td>\n",
       "      <td>2227.464111</td>\n",
       "      <td>5370.826172</td>\n",
       "      <td>2503.581543</td>\n",
       "      <td>5798.839355</td>\n",
       "      <td>2191.306152</td>\n",
       "      <td>...</td>\n",
       "      <td>291.375366</td>\n",
       "      <td>-1037.450439</td>\n",
       "      <td>288.512695</td>\n",
       "      <td>-1038.822876</td>\n",
       "      <td>286.387360</td>\n",
       "      <td>-1039.795288</td>\n",
       "      <td>282.346741</td>\n",
       "      <td>-1042.425171</td>\n",
       "      <td>278.503601</td>\n",
       "      <td>0_miami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-10182.574219</td>\n",
       "      <td>544.582031</td>\n",
       "      <td>-9606.491211</td>\n",
       "      <td>-135.450958</td>\n",
       "      <td>-8731.247070</td>\n",
       "      <td>-1338.176392</td>\n",
       "      <td>-8840.571289</td>\n",
       "      <td>-1108.741699</td>\n",
       "      <td>-8771.163086</td>\n",
       "      <td>-1316.220215</td>\n",
       "      <td>...</td>\n",
       "      <td>-1405.163696</td>\n",
       "      <td>-3402.020264</td>\n",
       "      <td>-1405.565186</td>\n",
       "      <td>-3401.718506</td>\n",
       "      <td>-1405.109741</td>\n",
       "      <td>-3401.263916</td>\n",
       "      <td>-1404.697266</td>\n",
       "      <td>-3400.469971</td>\n",
       "      <td>-1405.264404</td>\n",
       "      <td>1_miami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5339.036621</td>\n",
       "      <td>298.234222</td>\n",
       "      <td>5670.623047</td>\n",
       "      <td>214.221634</td>\n",
       "      <td>6023.845703</td>\n",
       "      <td>78.025017</td>\n",
       "      <td>6343.198242</td>\n",
       "      <td>323.120361</td>\n",
       "      <td>6343.576172</td>\n",
       "      <td>213.181381</td>\n",
       "      <td>...</td>\n",
       "      <td>78.633278</td>\n",
       "      <td>-1038.499634</td>\n",
       "      <td>66.356064</td>\n",
       "      <td>-1053.744385</td>\n",
       "      <td>53.246914</td>\n",
       "      <td>-1070.161133</td>\n",
       "      <td>44.140369</td>\n",
       "      <td>-1089.761719</td>\n",
       "      <td>43.972881</td>\n",
       "      <td>2_miami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4924.218262</td>\n",
       "      <td>713.216980</td>\n",
       "      <td>5079.324219</td>\n",
       "      <td>2283.355957</td>\n",
       "      <td>5272.743652</td>\n",
       "      <td>2813.492188</td>\n",
       "      <td>5519.493164</td>\n",
       "      <td>3089.389893</td>\n",
       "      <td>5635.434570</td>\n",
       "      <td>2845.612793</td>\n",
       "      <td>...</td>\n",
       "      <td>386.003113</td>\n",
       "      <td>-1066.353027</td>\n",
       "      <td>383.734711</td>\n",
       "      <td>-1067.662109</td>\n",
       "      <td>380.435577</td>\n",
       "      <td>-1068.895752</td>\n",
       "      <td>376.662476</td>\n",
       "      <td>-1071.426025</td>\n",
       "      <td>372.982910</td>\n",
       "      <td>3_miami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5555.431152</td>\n",
       "      <td>327.582794</td>\n",
       "      <td>5774.207031</td>\n",
       "      <td>183.565918</td>\n",
       "      <td>6067.661133</td>\n",
       "      <td>14.508665</td>\n",
       "      <td>6345.577637</td>\n",
       "      <td>232.783112</td>\n",
       "      <td>6302.487793</td>\n",
       "      <td>78.270432</td>\n",
       "      <td>...</td>\n",
       "      <td>60.653984</td>\n",
       "      <td>-1044.104736</td>\n",
       "      <td>48.031063</td>\n",
       "      <td>-1061.461426</td>\n",
       "      <td>35.421848</td>\n",
       "      <td>-1080.860474</td>\n",
       "      <td>30.604601</td>\n",
       "      <td>-1103.999268</td>\n",
       "      <td>37.299393</td>\n",
       "      <td>4_miami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7966</th>\n",
       "      <td>-1276.857544</td>\n",
       "      <td>2326.123779</td>\n",
       "      <td>14.248789</td>\n",
       "      <td>3107.310059</td>\n",
       "      <td>111.985504</td>\n",
       "      <td>3394.500000</td>\n",
       "      <td>133.314438</td>\n",
       "      <td>3921.260010</td>\n",
       "      <td>152.211487</td>\n",
       "      <td>3706.751221</td>\n",
       "      <td>...</td>\n",
       "      <td>2080.305908</td>\n",
       "      <td>-1244.254395</td>\n",
       "      <td>2038.868164</td>\n",
       "      <td>-1243.413696</td>\n",
       "      <td>1998.714233</td>\n",
       "      <td>-1245.107300</td>\n",
       "      <td>1960.082642</td>\n",
       "      <td>-1248.420166</td>\n",
       "      <td>1926.810181</td>\n",
       "      <td>7966_miami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7967</th>\n",
       "      <td>4391.926758</td>\n",
       "      <td>694.929688</td>\n",
       "      <td>4882.635254</td>\n",
       "      <td>1141.881470</td>\n",
       "      <td>5431.858887</td>\n",
       "      <td>1092.413940</td>\n",
       "      <td>5995.572754</td>\n",
       "      <td>1228.148682</td>\n",
       "      <td>6303.464355</td>\n",
       "      <td>1037.812988</td>\n",
       "      <td>...</td>\n",
       "      <td>173.261169</td>\n",
       "      <td>-1023.401917</td>\n",
       "      <td>167.358521</td>\n",
       "      <td>-1028.353149</td>\n",
       "      <td>160.466583</td>\n",
       "      <td>-1036.512695</td>\n",
       "      <td>154.450394</td>\n",
       "      <td>-1046.212158</td>\n",
       "      <td>147.629822</td>\n",
       "      <td>7967_miami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7968</th>\n",
       "      <td>-1806.880127</td>\n",
       "      <td>1131.236816</td>\n",
       "      <td>317.827454</td>\n",
       "      <td>1474.795288</td>\n",
       "      <td>675.820129</td>\n",
       "      <td>1521.112305</td>\n",
       "      <td>389.345001</td>\n",
       "      <td>1917.597046</td>\n",
       "      <td>19.851028</td>\n",
       "      <td>2221.679199</td>\n",
       "      <td>...</td>\n",
       "      <td>1631.986328</td>\n",
       "      <td>-1125.244385</td>\n",
       "      <td>1631.703735</td>\n",
       "      <td>-1128.062012</td>\n",
       "      <td>1630.967529</td>\n",
       "      <td>-1131.401978</td>\n",
       "      <td>1629.306641</td>\n",
       "      <td>-1135.251709</td>\n",
       "      <td>1626.793701</td>\n",
       "      <td>7968_miami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7969</th>\n",
       "      <td>-1187.430908</td>\n",
       "      <td>2978.611572</td>\n",
       "      <td>-101.979675</td>\n",
       "      <td>3946.589111</td>\n",
       "      <td>68.599579</td>\n",
       "      <td>4147.581543</td>\n",
       "      <td>172.599548</td>\n",
       "      <td>4529.883789</td>\n",
       "      <td>237.789154</td>\n",
       "      <td>4110.792480</td>\n",
       "      <td>...</td>\n",
       "      <td>2486.291748</td>\n",
       "      <td>-1393.583374</td>\n",
       "      <td>2431.482178</td>\n",
       "      <td>-1343.754883</td>\n",
       "      <td>2362.679932</td>\n",
       "      <td>-1302.115112</td>\n",
       "      <td>2281.249268</td>\n",
       "      <td>-1274.386108</td>\n",
       "      <td>2191.514893</td>\n",
       "      <td>7969_miami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7970</th>\n",
       "      <td>4785.062500</td>\n",
       "      <td>606.638794</td>\n",
       "      <td>4852.117188</td>\n",
       "      <td>2039.388062</td>\n",
       "      <td>5062.761230</td>\n",
       "      <td>2576.958252</td>\n",
       "      <td>5405.464355</td>\n",
       "      <td>2909.415283</td>\n",
       "      <td>5626.904297</td>\n",
       "      <td>2705.596680</td>\n",
       "      <td>...</td>\n",
       "      <td>359.427643</td>\n",
       "      <td>-1054.187622</td>\n",
       "      <td>357.282623</td>\n",
       "      <td>-1055.328491</td>\n",
       "      <td>354.939392</td>\n",
       "      <td>-1056.829224</td>\n",
       "      <td>352.119049</td>\n",
       "      <td>-1058.429077</td>\n",
       "      <td>349.633698</td>\n",
       "      <td>7970_miami</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7971 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                v0           v1           v2           v3           v4  \\\n",
       "0      4594.164551   593.487000  4679.035156  1782.869751  4877.182129   \n",
       "1    -10182.574219   544.582031 -9606.491211  -135.450958 -8731.247070   \n",
       "2      5339.036621   298.234222  5670.623047   214.221634  6023.845703   \n",
       "3      4924.218262   713.216980  5079.324219  2283.355957  5272.743652   \n",
       "4      5555.431152   327.582794  5774.207031   183.565918  6067.661133   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "7966  -1276.857544  2326.123779    14.248789  3107.310059   111.985504   \n",
       "7967   4391.926758   694.929688  4882.635254  1141.881470  5431.858887   \n",
       "7968  -1806.880127  1131.236816   317.827454  1474.795288   675.820129   \n",
       "7969  -1187.430908  2978.611572  -101.979675  3946.589111    68.599579   \n",
       "7970   4785.062500   606.638794  4852.117188  2039.388062  5062.761230   \n",
       "\n",
       "               v5           v6           v7           v8           v9  ...  \\\n",
       "0     2227.464111  5370.826172  2503.581543  5798.839355  2191.306152  ...   \n",
       "1    -1338.176392 -8840.571289 -1108.741699 -8771.163086 -1316.220215  ...   \n",
       "2       78.025017  6343.198242   323.120361  6343.576172   213.181381  ...   \n",
       "3     2813.492188  5519.493164  3089.389893  5635.434570  2845.612793  ...   \n",
       "4       14.508665  6345.577637   232.783112  6302.487793    78.270432  ...   \n",
       "...           ...          ...          ...          ...          ...  ...   \n",
       "7966  3394.500000   133.314438  3921.260010   152.211487  3706.751221  ...   \n",
       "7967  1092.413940  5995.572754  1228.148682  6303.464355  1037.812988  ...   \n",
       "7968  1521.112305   389.345001  1917.597046    19.851028  2221.679199  ...   \n",
       "7969  4147.581543   172.599548  4529.883789   237.789154  4110.792480  ...   \n",
       "7970  2576.958252  5405.464355  2909.415283  5626.904297  2705.596680  ...   \n",
       "\n",
       "             v111         v112         v113         v114         v115  \\\n",
       "0      291.375366 -1037.450439   288.512695 -1038.822876   286.387360   \n",
       "1    -1405.163696 -3402.020264 -1405.565186 -3401.718506 -1405.109741   \n",
       "2       78.633278 -1038.499634    66.356064 -1053.744385    53.246914   \n",
       "3      386.003113 -1066.353027   383.734711 -1067.662109   380.435577   \n",
       "4       60.653984 -1044.104736    48.031063 -1061.461426    35.421848   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "7966  2080.305908 -1244.254395  2038.868164 -1243.413696  1998.714233   \n",
       "7967   173.261169 -1023.401917   167.358521 -1028.353149   160.466583   \n",
       "7968  1631.986328 -1125.244385  1631.703735 -1128.062012  1630.967529   \n",
       "7969  2486.291748 -1393.583374  2431.482178 -1343.754883  2362.679932   \n",
       "7970   359.427643 -1054.187622   357.282623 -1055.328491   354.939392   \n",
       "\n",
       "             v116         v117         v118         v119          ID  \n",
       "0    -1039.795288   282.346741 -1042.425171   278.503601     0_miami  \n",
       "1    -3401.263916 -1404.697266 -3400.469971 -1405.264404     1_miami  \n",
       "2    -1070.161133    44.140369 -1089.761719    43.972881     2_miami  \n",
       "3    -1068.895752   376.662476 -1071.426025   372.982910     3_miami  \n",
       "4    -1080.860474    30.604601 -1103.999268    37.299393     4_miami  \n",
       "...           ...          ...          ...          ...         ...  \n",
       "7966 -1245.107300  1960.082642 -1248.420166  1926.810181  7966_miami  \n",
       "7967 -1036.512695   154.450394 -1046.212158   147.629822  7967_miami  \n",
       "7968 -1131.401978  1629.306641 -1135.251709  1626.793701  7968_miami  \n",
       "7969 -1302.115112  2281.249268 -1274.386108  2191.514893  7969_miami  \n",
       "7970 -1056.829224   352.119049 -1058.429077   349.633698  7970_miami  \n",
       "\n",
       "[7971 rows x 121 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_miami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_result.to_csv('seq_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v0</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v111</th>\n",
       "      <th>v112</th>\n",
       "      <th>v113</th>\n",
       "      <th>v114</th>\n",
       "      <th>v115</th>\n",
       "      <th>v116</th>\n",
       "      <th>v117</th>\n",
       "      <th>v118</th>\n",
       "      <th>v119</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>962.567932</td>\n",
       "      <td>-693.372498</td>\n",
       "      <td>-49.638153</td>\n",
       "      <td>-801.457153</td>\n",
       "      <td>112.891197</td>\n",
       "      <td>-709.244629</td>\n",
       "      <td>258.044312</td>\n",
       "      <td>-671.463013</td>\n",
       "      <td>96.557739</td>\n",
       "      <td>-761.679382</td>\n",
       "      <td>...</td>\n",
       "      <td>1308.728027</td>\n",
       "      <td>-591.067078</td>\n",
       "      <td>1307.620239</td>\n",
       "      <td>-602.068909</td>\n",
       "      <td>1303.947876</td>\n",
       "      <td>-620.871460</td>\n",
       "      <td>1304.786499</td>\n",
       "      <td>-650.387085</td>\n",
       "      <td>1314.858765</td>\n",
       "      <td>0_austin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>319.805450</td>\n",
       "      <td>141.750580</td>\n",
       "      <td>-244.078934</td>\n",
       "      <td>-155.169373</td>\n",
       "      <td>-140.587692</td>\n",
       "      <td>-428.437592</td>\n",
       "      <td>-229.159897</td>\n",
       "      <td>-320.734680</td>\n",
       "      <td>-414.009247</td>\n",
       "      <td>-272.971680</td>\n",
       "      <td>...</td>\n",
       "      <td>1293.444580</td>\n",
       "      <td>-635.186707</td>\n",
       "      <td>1288.675781</td>\n",
       "      <td>-642.735229</td>\n",
       "      <td>1283.478882</td>\n",
       "      <td>-654.868469</td>\n",
       "      <td>1287.385132</td>\n",
       "      <td>-671.549316</td>\n",
       "      <td>1313.055786</td>\n",
       "      <td>1_austin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1159.914673</td>\n",
       "      <td>-71.822998</td>\n",
       "      <td>99.772217</td>\n",
       "      <td>-395.863647</td>\n",
       "      <td>129.435059</td>\n",
       "      <td>-549.387939</td>\n",
       "      <td>140.644318</td>\n",
       "      <td>-576.463928</td>\n",
       "      <td>-57.491364</td>\n",
       "      <td>-597.404541</td>\n",
       "      <td>...</td>\n",
       "      <td>1324.049072</td>\n",
       "      <td>-552.522400</td>\n",
       "      <td>1324.143677</td>\n",
       "      <td>-561.729797</td>\n",
       "      <td>1319.239380</td>\n",
       "      <td>-578.835632</td>\n",
       "      <td>1319.523438</td>\n",
       "      <td>-605.876892</td>\n",
       "      <td>1322.520508</td>\n",
       "      <td>2_austin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-80.409050</td>\n",
       "      <td>1463.862915</td>\n",
       "      <td>52.217545</td>\n",
       "      <td>1940.990112</td>\n",
       "      <td>-329.069794</td>\n",
       "      <td>2025.319580</td>\n",
       "      <td>-461.468689</td>\n",
       "      <td>2100.540771</td>\n",
       "      <td>-62.703194</td>\n",
       "      <td>2194.368896</td>\n",
       "      <td>...</td>\n",
       "      <td>1952.025635</td>\n",
       "      <td>-138.920547</td>\n",
       "      <td>1951.281006</td>\n",
       "      <td>-139.925476</td>\n",
       "      <td>1949.994263</td>\n",
       "      <td>-141.849823</td>\n",
       "      <td>1949.605469</td>\n",
       "      <td>-143.099426</td>\n",
       "      <td>1948.032715</td>\n",
       "      <td>3_austin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2054.545654</td>\n",
       "      <td>-734.349182</td>\n",
       "      <td>1226.631714</td>\n",
       "      <td>-967.156860</td>\n",
       "      <td>1156.185913</td>\n",
       "      <td>-934.666687</td>\n",
       "      <td>1118.501099</td>\n",
       "      <td>-783.604370</td>\n",
       "      <td>1341.803101</td>\n",
       "      <td>-658.240601</td>\n",
       "      <td>...</td>\n",
       "      <td>1393.031616</td>\n",
       "      <td>-424.461670</td>\n",
       "      <td>1392.602905</td>\n",
       "      <td>-428.318237</td>\n",
       "      <td>1397.749146</td>\n",
       "      <td>-438.104126</td>\n",
       "      <td>1417.502563</td>\n",
       "      <td>-457.325439</td>\n",
       "      <td>1439.801514</td>\n",
       "      <td>4_austin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>-1247.900391</td>\n",
       "      <td>-455.744263</td>\n",
       "      <td>-1212.770630</td>\n",
       "      <td>-173.395569</td>\n",
       "      <td>-1267.413818</td>\n",
       "      <td>-294.179321</td>\n",
       "      <td>-1553.954224</td>\n",
       "      <td>-318.581177</td>\n",
       "      <td>-1505.412720</td>\n",
       "      <td>-518.760986</td>\n",
       "      <td>...</td>\n",
       "      <td>370.255402</td>\n",
       "      <td>-50.322712</td>\n",
       "      <td>377.707184</td>\n",
       "      <td>-53.932964</td>\n",
       "      <td>385.394409</td>\n",
       "      <td>-57.831406</td>\n",
       "      <td>392.847351</td>\n",
       "      <td>-61.276852</td>\n",
       "      <td>400.207947</td>\n",
       "      <td>1681_palo-alto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>147.834427</td>\n",
       "      <td>-573.721558</td>\n",
       "      <td>267.254120</td>\n",
       "      <td>-320.408630</td>\n",
       "      <td>385.596741</td>\n",
       "      <td>-178.161270</td>\n",
       "      <td>500.868866</td>\n",
       "      <td>-220.734482</td>\n",
       "      <td>352.692291</td>\n",
       "      <td>-406.737640</td>\n",
       "      <td>...</td>\n",
       "      <td>198.004837</td>\n",
       "      <td>247.138443</td>\n",
       "      <td>204.924942</td>\n",
       "      <td>244.644440</td>\n",
       "      <td>212.825531</td>\n",
       "      <td>243.543381</td>\n",
       "      <td>220.286255</td>\n",
       "      <td>242.031662</td>\n",
       "      <td>227.014420</td>\n",
       "      <td>1682_palo-alto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>-1423.583374</td>\n",
       "      <td>2060.592285</td>\n",
       "      <td>-1573.127563</td>\n",
       "      <td>1980.399780</td>\n",
       "      <td>-1279.949829</td>\n",
       "      <td>2206.351318</td>\n",
       "      <td>-1506.974976</td>\n",
       "      <td>2273.049561</td>\n",
       "      <td>-1504.950806</td>\n",
       "      <td>2139.585205</td>\n",
       "      <td>...</td>\n",
       "      <td>2333.726562</td>\n",
       "      <td>-422.979309</td>\n",
       "      <td>2337.493652</td>\n",
       "      <td>-423.184631</td>\n",
       "      <td>2341.053467</td>\n",
       "      <td>-423.539581</td>\n",
       "      <td>2344.164062</td>\n",
       "      <td>-423.715698</td>\n",
       "      <td>2348.487305</td>\n",
       "      <td>1683_palo-alto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>1138.744629</td>\n",
       "      <td>1111.202393</td>\n",
       "      <td>934.236328</td>\n",
       "      <td>1273.613525</td>\n",
       "      <td>823.001831</td>\n",
       "      <td>1326.743530</td>\n",
       "      <td>740.325134</td>\n",
       "      <td>1431.362061</td>\n",
       "      <td>775.268250</td>\n",
       "      <td>1495.653564</td>\n",
       "      <td>...</td>\n",
       "      <td>1276.496704</td>\n",
       "      <td>1082.242432</td>\n",
       "      <td>1272.352905</td>\n",
       "      <td>1081.021973</td>\n",
       "      <td>1266.977539</td>\n",
       "      <td>1079.172485</td>\n",
       "      <td>1260.531860</td>\n",
       "      <td>1077.735962</td>\n",
       "      <td>1255.046143</td>\n",
       "      <td>1684_palo-alto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>194.517380</td>\n",
       "      <td>304.665558</td>\n",
       "      <td>-64.941940</td>\n",
       "      <td>261.870331</td>\n",
       "      <td>-222.796204</td>\n",
       "      <td>373.923492</td>\n",
       "      <td>-26.485920</td>\n",
       "      <td>489.028107</td>\n",
       "      <td>63.501297</td>\n",
       "      <td>191.141068</td>\n",
       "      <td>...</td>\n",
       "      <td>490.475891</td>\n",
       "      <td>-76.023636</td>\n",
       "      <td>492.661896</td>\n",
       "      <td>-76.992210</td>\n",
       "      <td>493.673676</td>\n",
       "      <td>-77.559952</td>\n",
       "      <td>494.998383</td>\n",
       "      <td>-76.102318</td>\n",
       "      <td>496.497467</td>\n",
       "      <td>1685_palo-alto</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29843 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               v0           v1           v2           v3           v4  \\\n",
       "0      962.567932  -693.372498   -49.638153  -801.457153   112.891197   \n",
       "1      319.805450   141.750580  -244.078934  -155.169373  -140.587692   \n",
       "2     1159.914673   -71.822998    99.772217  -395.863647   129.435059   \n",
       "3      -80.409050  1463.862915    52.217545  1940.990112  -329.069794   \n",
       "4     2054.545654  -734.349182  1226.631714  -967.156860  1156.185913   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1681 -1247.900391  -455.744263 -1212.770630  -173.395569 -1267.413818   \n",
       "1682   147.834427  -573.721558   267.254120  -320.408630   385.596741   \n",
       "1683 -1423.583374  2060.592285 -1573.127563  1980.399780 -1279.949829   \n",
       "1684  1138.744629  1111.202393   934.236328  1273.613525   823.001831   \n",
       "1685   194.517380   304.665558   -64.941940   261.870331  -222.796204   \n",
       "\n",
       "               v5           v6           v7           v8           v9  ...  \\\n",
       "0     -709.244629   258.044312  -671.463013    96.557739  -761.679382  ...   \n",
       "1     -428.437592  -229.159897  -320.734680  -414.009247  -272.971680  ...   \n",
       "2     -549.387939   140.644318  -576.463928   -57.491364  -597.404541  ...   \n",
       "3     2025.319580  -461.468689  2100.540771   -62.703194  2194.368896  ...   \n",
       "4     -934.666687  1118.501099  -783.604370  1341.803101  -658.240601  ...   \n",
       "...           ...          ...          ...          ...          ...  ...   \n",
       "1681  -294.179321 -1553.954224  -318.581177 -1505.412720  -518.760986  ...   \n",
       "1682  -178.161270   500.868866  -220.734482   352.692291  -406.737640  ...   \n",
       "1683  2206.351318 -1506.974976  2273.049561 -1504.950806  2139.585205  ...   \n",
       "1684  1326.743530   740.325134  1431.362061   775.268250  1495.653564  ...   \n",
       "1685   373.923492   -26.485920   489.028107    63.501297   191.141068  ...   \n",
       "\n",
       "             v111         v112         v113         v114         v115  \\\n",
       "0     1308.728027  -591.067078  1307.620239  -602.068909  1303.947876   \n",
       "1     1293.444580  -635.186707  1288.675781  -642.735229  1283.478882   \n",
       "2     1324.049072  -552.522400  1324.143677  -561.729797  1319.239380   \n",
       "3     1952.025635  -138.920547  1951.281006  -139.925476  1949.994263   \n",
       "4     1393.031616  -424.461670  1392.602905  -428.318237  1397.749146   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1681   370.255402   -50.322712   377.707184   -53.932964   385.394409   \n",
       "1682   198.004837   247.138443   204.924942   244.644440   212.825531   \n",
       "1683  2333.726562  -422.979309  2337.493652  -423.184631  2341.053467   \n",
       "1684  1276.496704  1082.242432  1272.352905  1081.021973  1266.977539   \n",
       "1685   490.475891   -76.023636   492.661896   -76.992210   493.673676   \n",
       "\n",
       "             v116         v117         v118         v119              ID  \n",
       "0     -620.871460  1304.786499  -650.387085  1314.858765        0_austin  \n",
       "1     -654.868469  1287.385132  -671.549316  1313.055786        1_austin  \n",
       "2     -578.835632  1319.523438  -605.876892  1322.520508        2_austin  \n",
       "3     -141.849823  1949.605469  -143.099426  1948.032715        3_austin  \n",
       "4     -438.104126  1417.502563  -457.325439  1439.801514        4_austin  \n",
       "...           ...          ...          ...          ...             ...  \n",
       "1681   -57.831406   392.847351   -61.276852   400.207947  1681_palo-alto  \n",
       "1682   243.543381   220.286255   242.031662   227.014420  1682_palo-alto  \n",
       "1683  -423.539581  2344.164062  -423.715698  2348.487305  1683_palo-alto  \n",
       "1684  1079.172485  1260.531860  1077.735962  1255.046143  1684_palo-alto  \n",
       "1685   -77.559952   494.998383   -76.102318   496.497467  1685_palo-alto  \n",
       "\n",
       "[29843 rows x 121 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import random\n",
    "\n",
    "\n",
    "# def show_sample_batch(sample_batch):\n",
    "#     \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "#     inp, out = sample_batch\n",
    "#     batch_sz = inp.size(0)\n",
    "#     agent_sz = inp.size(1)\n",
    "    \n",
    "#     fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "#     fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "#     axs = axs.ravel()   \n",
    "#     for i in range(batch_sz):\n",
    "#         axs[i].xaxis.set_ticks([])\n",
    "#         axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "#         # first two feature dimensions are (x,y) positions\n",
    "#         axs[i].scatter(inp[i,:,0], inp[i,:,1])\n",
    "#         axs[i].scatter(out[i,:,0], out[i,:,1])\n",
    "\n",
    "        \n",
    "# for i_batch, sample_batch in enumerate(train_loader):\n",
    "#     inp, out = sample_batch\n",
    "#     \"\"\"\n",
    "#     TODO:\n",
    "#       implement your Deep learning model\n",
    "#       implement training routine\n",
    "#     \"\"\"\n",
    "#     show_sample_batch(sample_batch)\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81df09b6fc329197c79c0fb00fc4d302ba91ba358ece85a8e43db13ca08453ff"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 ('data_analysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
